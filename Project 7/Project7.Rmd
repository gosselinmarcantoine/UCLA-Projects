---
title: "Project 6 (week 7)"
author: "Marc-Antoine Gosselin"
date: "11/17/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE, error=FALSE)
```

To run this R markdown file, the following libraries must be installed and uploaded:
```{r libraries}
library(readxl)
library(purrr)
library(tidyverse)
library(plm)
library(imputeTS)
library(corrplot)
library(caret)
library(glmnet)
library(mlbench)
library(psych)
library(VIM)
library(foreign)
library(car)
library(prediction)
```

Our goal in this file is to identify the best five variables to predict the dependent variable: 'Poverty gap at $3.20 a day (2012 PPP) (%)' and to make a simple forcast for the year of 2018.

Let's take a look at our data.

Percentage of missing values per column:
```{r Missing Values}
df <- read_excel('W03b_wdi.xlsx') # Loading the data
map_dbl(df, ~sum(is.na(.))/ nrow(df)) # Lots of missing values (2017 seems to have partial data)
```

Number of variables (Country Name & Code, Indicator Name & Code):
```{r Number of countries & variables}
map_dbl(df[, 1:4], ~length(unique(.))) # 1591 variables: data is not tidy
df$`Country Code` <- factor(df$`Country Code`)
```

Two main problems can be seen here, namely that there is a significant missing values issue that are not random, and that the data is not tidy (one observation per row and each column is a variable and no duplicate information).

## Pre-processing

Steps to wrangle our data:

* We simplify our dataframe by removing duplicate columns (removing names/ labels and leaving codes). We will create two new tables, one for the indicators and one for the countries with the associated name and code for later reference.

```{r Reference tables}
indicator_reference <- subset(df, select = c("Indicator Name", "Indicator Code")) %>% distinct()
df <- subset(df, select = -`Indicator Name`)
Country_reference <- subset(df, select = c("Country Name", "Country Code")) %>% distinct()
df <- subset(df, select = -`Country Name`)
```

* Remove countries without a value for Indicator Code: SI.POV.LMIC.GP in 2012.
```{r Conforming dataset}
CountryCodes <- filter(df[df[, 2] == 'SI.POV.LMIC.GP', 1], df[df[, 2] == 'SI.POV.LMIC.GP', 55] != 'NA')
df <- subset(df, `Country Code` %in% CountryCodes$`Country Code`)
```

* Remove years 1960 to 1980, and 2017 (2017 has partial data).
```{r Remove years 1960-1980}
df <- subset(df, select = -c(`1960`:`1980`, `2017`))
```

* Separates dataframe into training and testing sets.
```{r split train/test}
df_test <- subset(df, select = c(1:2, 35:38)) # Excludes 2017 as well
df <- subset(df, select = 1:34)
```


* Gathering time columns into one year column.
```{r Gather}
df <- df %>% gather(key = 'Year', value = 'value', c(3:34)) # %>% filter(value != 'NA') 
df_test <- df_test %>% gather(key = 'Year', value = 'value', c(3:6))
df$Year <- factor(df$Year)
```


* Spread Indicator code to columns
```{r Spread}
df <- df %>% spread(key = 'Indicator Code', value = 'value')
df_test <- df_test %>% spread(key = 'Indicator Code', value = 'value')
```


* Remove Indicators with over 25% missing values in training set
```{r Remove bad indicators}
dep_var <- subset(df, select = 'SI.POV.LMIC.GP')
df <- subset(df, select = map_dbl(df, ~sum(is.na(.))/ nrow(df)) <= .25)
df <- cbind(df, dep_var)
df_test <- subset(df_test, select = names(df)) # Selecting the same variables as the training dataset
```

The end result of pre-processing (limiting our data to four indicators out of 1591) returns:
```{r Dataset after pre-processing, fig.width=16}
head(df[, 1:6], 2)
```


## Variable Selection 

Imputation for all our data is too intensive to perform on a regular computer. Instead we will reduce the size of our data to easily go through the computation related to variable selection, then, once we have the few variables of interest, we'll impute the missing values for our defined variables from the original data.

We'll restrict the dataset to 2007 and 2012, imputing missing values for all except our dependent variable, because imputing the dependent variable could affect our model negatively. We include two years because the data contains missing values for all of the countries in the 2009-2012 period. I chose not to include 2008 because our variable of interest relates to the economy and 2008 was a special year resulting from a world-wide recession that could lead to non-regular behavior in our model.
```{r Variable selection pre-processing}
df_x <- df %>% filter(df[, 2] %in% c('2012', '2007')) # Add 2007 because some variables don't have values from 2009 - 2012. Choosing 2007 because we want a year that is similar to the test set 2013-2017 and 2008 differs considerably because of a major world fiancial collapse.
df_x <- kNN(df_x, variable = names(df_x[, 3:321]), k = 3, imp_var = FALSE) # kNN imputation
```

Filter methods do not adress multi-collinearity problems well and wrapper methods handles large numbers of explanatory variables poorly (long run-time). As a result, I will use an embbeded method for variable selection. These methods include: Ridge, LASSO, and Elastic Net. *Note: we expect LASSO (or elastic net leaning towards lasso) to be the appropriate technique because we know our variable selection have great multicollinearity*.

First we have to check our assumptions. In this case that the dependent variable has a normal distribution.
```{r Dependent variable examination}
hist(df$SI.POV.LMIC.GP) 
```

Obviousy, SI.POV.LMIC.GP is a non-normal distribution so we cannot use a linear model. We'll have to use GLM instead.

Variable selection techniques:

```{r variable selection}
custom <- trainControl(method = 'repeatedcv',
                       number = 10,
                       repeats = 5,
                       verboseIter = FALSE) # if verboseIter = TRUE, See the model running 

# glm models
set.seed(12)
ridge <- train(SI.POV.LMIC.GP ~ .,
               df_x[3:322],
               method = 'glmnet',
               tuneGrid = expand.grid(alpha = 0, # alpha = 0: ridge | alpha = 1: lasso | alpha >0 & <1 means elastic net
                                      lambda =  seq(0.0001, 1, length = 5)),
               trControl = custom,
               na.action = na.omit)

set.seed(12)
lasso <- train(SI.POV.LMIC.GP ~ .,
               df_x[3:322],
               method = 'glmnet',
               tuneGrid = expand.grid(alpha = 1, # alpha = 0: ridge | alpha = 1: lasso | alpha >0 & <1 means elastic net
                                      lambda =  seq(0.0001, 1, length = 5)),
               trControl = custom,
               na.action = na.omit) 

set.seed(12)
elasticnet <- train(SI.POV.LMIC.GP ~ .,
             df_x[3:322],
             method = 'glmnet',
             tuneGrid = expand.grid(alpha = seq(0, 1, length = 10), # alpha = 0: ridge | alpha = 1: lasso | alpha >0 & <1 means elastic net
                                    lambda =  seq(0.0001, 1, length = 5)),
             trControl = custom,
             na.action = na.omit) 

# Compare Models
model_list <- list(Ridge = ridge, Lasso = lasso, ElasticNet = elasticnet)
res <- resamples(model_list)
summary(res)
```

Elastic Net is a slightly better model. Let's look at some visualisations.

```{r}
elasticnet$bestTune
plot(elasticnet$finalModel, xvar = 'lambda', label = T)
```
This graph shows the number of non-zero variables (top axis) for the lambda value (bottom axis). For example, for lambda = 1, there are 63 non-zero variables. This is pretty good considering we started with 1591 indicators/ variables.

```{r}
plot(elasticnet$finalModel, xvar = 'dev', label = T) # We see great risks of overfitting past .9
```
This graph shows the variance explained (lower axis) by the number of variables selected (upper axis). This graph also shows that past 80% of the variance explained, there is potential for overfitting as the variables lose their monotonical properties.

```{r}
plot(varImp(elasticnet, scale = F))
```
The graphs above show that there is an exponential trend in explanatory importance for the variables.

Let's take a look at the first 34 variables.

Five most informative explanatory variables:
```{r Five best variables}
imp <- data.frame(varImp(elasticnet, scale = F)[1])
(vars <- rownames(imp)[order(imp$Overall, decreasing=TRUE)[1:34]])
```

Five best explanatory variables by name:
```{r Names of explanatory variables}
filter(indicator_reference, `Indicator Code` %in% vars)
```

## Data Formatting

Before proceeding with the reduced and imputed original dataset, let's take a look at the data's structure.
```{r Variable selection to original dataset}
df <- subset(df, select = c("Country Code", "Year", "SI.POV.LMIC.GP", vars))
dependent_variable <- df_test$SI.POV.LMIC.GP
df_test <- subset(df_test, select = c("Country Code", "Year", vars))

df <- kNN(df, variable = vars, k = 5, imp_var = FALSE)
df_test <- kNN(df_test, variable = vars, k = 5, imp_var = FALSE)

df <- rename(df, 'Country' = 'Country Code')
df_test <- rename(df_test, 'Country' = 'Country Code')

head(df, 2)
```


## Panel Data Analysis Model

Panel data modelling makes sense here because our time variable is repartitionned across another variable, in this case, countries. 

Here is a great video to explain how to handle panel data analysis: *https://www.youtube.com/watch?v=f01WjeCdgEA*. And for step-by-step help: *https://www.princeton.edu/~otorres/Panel101R.pdf*

We'll be using the best 34 variables to capture around 80% of the variability. Running a quick regsubsets function shows that interactions are not amongst the most important elements, so we will not include them.
```{r Modelling, results='hide'}

# Linear Model
linear_model <- lm(
  as.formula(paste0("SI.POV.LMIC.GP ~ ", paste(vars, collapse = " + "))),
  data = df)

# Fixed effect panel data model
fxd_effect <- plm(
  as.formula(paste0("SI.POV.LMIC.GP ~ ", "Country + ", paste(vars, collapse = " + "))),
  data = df,
  method = "within",
  index = c("Country", "Year"))

# Random effect panel model
random <- plm(
  as.formula(paste0("SI.POV.LMIC.GP ~ ", paste(vars, collapse = " + "))),
  data = df,
  method = "within",
  model = "random",
  index = c("Country", "Year"))

# Time effect panel data model
time_effect <- plm(
  as.formula(paste0("SI.POV.LMIC.GP ~ ", "lag(Year, 1) + ", paste(vars, collapse = " + "))),
  data = df,
  method = "within",
  effect = "time",
  index = c("Country", "Year")) # Fixed effect model
```

The five elements included in our models are:

1. SP.DYN.TFRT.IN

2. NY.GDP.FRST.RT.ZS:EN.ATM.CO2E.KD.GD

3. SP.DYN.TFRT.IN:TM.VAL.MRCH.R6.ZS

4. AG.LND.ARBL.HA.PC:NY.GDP.FRST.RT.ZS:EN.ATM.CO2E.KD.GD 

5. NY.GDP.FRST.RT.ZS:EN.ATM.CO2E.KD.GD:TM.VAL.MRCH.R6.ZS

Let's compare our models.
```{r Model Selection}
linear <- summary(linear_model) # linear model is usually the basis for comparison
linear$adj.r.squared

fxd <- summary(fxd_effect)
fxd$r.squared

# Comparing the models
pFtest(fxd_effect, linear_model) # P-value is small, fxd_effect is better.

rand <- summary(random)
rand$r.squared

# Choice b/w Fixed Effect and Random Effect
# Hausman Test
phtest(fxd_effect, random)  # If p-value is <0.05, then the fixed effect model is better than random effect!
pFtest(fxd_effect, random)  # we keep the fixed effect

time <- summary(time_effect)
time$r.squared
```

Panel data with time effect seems to be the most appropriate model for our data. The Linear model obtained a higher R-squared value but since we know that only about 80% of the variability should be explained from the 34 variables, we also know that the linear model overfits our model. However, prediction(time_effect, data = df_test, at = NULL, calculate_se = FALSE) returns the following error: Error in crossprod(beta, t(X)) : non-conformable arguments. Unfortunately, forecasting with panel data is complex and there are no good function presently in R that enables forecasting from unbalanced panel data models. Panel data with random effect can be used but we just showed that our model does not have a fixed effect.

For those reasons, we will go back to our linear model and hope that the overfitting is not large. 

```{r Prediction accuracy}
# Model Prediction --------------------------------------------------------

# Linear model forecast
LMprediction <- predict(linear_model, df_test)

Predicted <- cbind(df_test, Actual = dependent_variable, Prediction = LMprediction)

# Prediction accuracy
Predicted$Prediction <- as.numeric(Predicted$Prediction)
Predicted$Actual <- as.numeric(Predicted$Actual)
Predicted <- Predicted %>% mutate(Error = Actual - Prediction)
```

The sum of squared errors is `r sqrt(sum(Predicted$Error^2, na.rm = TRUE))` for `r sum(!is.na(Predicted$Error))` predictions with a value. 